# WebRTC解析

## 一. WebRTC之peerconnection

### 1.1 peerconnection大体的结构类图

<img src="WebRTC手记之框架与接口.assets/365763e739e71c89b4f148affe91ac49.svg" alt="img" style="zoom:150%;" />

### 1.2 Conductor类

从上图中可以看出，整个例子是以Conductor类为中心去进行开展的，其中Coductor泛化了PeerConnectionClientObserver, MainWndCallBack, PeerConnectionObserver , CreateSessionDescriptionObserver这四个父类，然后实例化继承的函数，这在设计模式中叫做观察者模式。

这里比较特殊的是使用了一个DummySetSessionDescriptionObserver，这是一个假的设置SDP信息观察者，没有做什么实际的操作，只是将其传入到对应的函数中。

| 基类                             | 说明                                                   |
| -------------------------------- | ------------------------------------------------------ |
| MainWndCallBack                  | 界面操作回调，主要响应点击连接等事件                   |
| PeerConnectionObserver           | PC连接观察者，用来通知PeerConnection中的各种状态，数据 |
| CreateSessionDescriptionObserver | 创建SDP信息观察者，主要就是通知成功或失败              |
| PeerConnectionClientObserver     | 自定义信令连接观察者，通知各种自定义消息，例如SDP信息  |

同时Coductor包含了MainWindow, PeerConnectionClient, PeerConnection, PeerConnectionFactoy等类的实例，用来进行各种实际的任务调度。

|                      |                                                         |
| -------------------- | ------------------------------------------------------- |
| MainWindow           | 界面显示控制，点击                                      |
| PeerConnectionClient | 自定义的信令通道，例子中使用TCP实现                     |
| PeerConnection       | PC连接类，包括PC通信中的各个功能                        |
| PeerConnectionFactoy | 创建PeerConnection, 创建音视频轨迹(track, 这个后面再说) |

还有一部分就是使用了CaptureTrackSource这个类，这个类中包含了VcmCapturer，实现了视频捕捉，同时继承了VideoSourceInterface, 用来管理视频输出端。

### 1.3 视频捕捉

![视频捕捉](WebRTC手记之框架与接口.assets/视频捕捉.jpg)

上图中比较重要的两个类为DeviceInfo 和 VideoCaptureModule， 通过VideoCaptureFactory可以创建这两个类的实例。

| 类                 | 功能         |
| ------------------ | ------------ |
| DeviceInfo         | 设备信息管理 |
| VideoCaptureModule | 视频捕捉     |

其中DeviceInfoDS和VideoCaptureDS是在window平台使用的， DeviceInfoLinux和VideoCaptureModuleV4L2是在Linux平台使用。

![视频捕捉时序图](WebRTC手记之框架与接口.assets/视频捕捉时序图.jpg)

视频捕捉的流程如上图所示: 

1. VcmCapturer创建一个VideoCaptureModuleV4L2
2. 因为VcmCapturer继承于VideoSinkInterface, 所以VcmCapturer将自身注册回调函数。
3. 调用StartCapture。
4. VideoCaptureModuleV4L2捕捉到视频后触发VcmCapturer的OnFrame函数
5. 然后调用TestVideoCapturer的OnFrame函数，再像外传输。

### 1.5 tracksource、track、sink、mediastream

在说本地视频渲染之前，先说明几个概念:  tracksource，track，sink。

track: 轨道，每个轨道代表一路媒体流，可以是音频流或者视频流。

tracksource: 轨道数据源， 轨道媒体数据的来源，很好理解， 各个媒体源都是继承VideoTrackSource。

sink: 轨道槽， 轨道媒体数据的输出，这个也很好理解，但是sink可以有多个。

mediastream: 媒体流

综合来讲: 一个媒体流中会有多个track, 创建track是必须绑定一个tracksource,   通过调用track的AddOrUpdateSink函数，进而调用tracksouce的AddOrUpdateSink函数注册sink， tracksource收到数据后调用sink输出数据。

#### 1.5.1 tracksource

![tracksource](WebRTC手记之框架与接口.assets/tracksource.jpg)

tracksource其实不是真正的媒体流来源，而是从媒体源获取数据后充当track的数据源。

VideoTrackSource中source()为纯虚函数，需要继承的子类去实现。然后再调用AddOrUpdateSink会调用子类实现的source中的AddOrUpdateSink， 这样就将sink注册到真实的源中。

其中子类中CapturerTrackSource是负责localtracksource, VideoRtpTrackSource负责remotetracksource。

还有一个比较重要的类为VideoBroadcaster， 继承了VideoSourceInterface和VideoSinkInterface,  具备管理sink和充当sink的功能，当作sink使用时作用是将自己本身注册到真实的源中，通过onFrame获取媒体数据，然后调用来自于VideoSourceInterface的sink_pair,将获取到的数据广播到各个sink中， 是一个非常重要的中转类。

#### 1.5.2 track
![track](WebRTC手记之框架与接口.assets/track.jpg)

VideoTrack依赖VideoTrackSoureInterFace和VideoSourceInterface。

所以VideoTrack本身是具备sink管理的功能，不过这部分功能只是根据媒体流的状态选择是否要重新注册sink, 其他的功能还是有VideoTrackSourceInterface去完成。

#### 1.5.3 sink

这块没什么好说的，只要继承VideoSinkInterface就可以作为sink，通过OnFrame获取数据, sink无处不在。

#### 1.5.4 mediastream

如下所示: 主要实现了track增删查找

![ClassDiagram1](WebRTC手记之框架与接口.assets/ClassDiagram1.jpg)

### 1.4 本地视频渲染

![本地视频渲染](WebRTC手记之框架与接口.assets/本地视频渲染.jpg)

VideoRender继承VideoSinkInterFace, 将本身作为sink，通过调用videoTrack的AddOrUpdateSink注册自己。

CaptureTrackSource包含了VcmVapturer, 而VcmVapture继承了TestVideoVapturer ， TestVideoVapturer使用VideoBroadcaster, 所以capturerTrackSource间接使用了VideoBroadcaster去管理sink， 具体流程看下面的流程图。

videotrack构造时将CaptureTrackSource作为tracksource输入。

![本地视频渲染时序图](WebRTC手记之框架与接口.assets/本地视频渲染时序图.jpg)

### 1.5 p2p连接

在讲远程视频获取之前，先讲p2p连接。

![](WebRTC手记之框架与接口.assets/250050215053233.png)

如上图所示，这里我们具体解析每一步peerconnection做了什么。

#### 1.5.1 create peerconnection

![1575344773686](WebRTC手记之框架与接口.assets/1575344773686.png)

![1575339600968](WebRTC手记之框架与接口.assets/1575339600968.png)

1. 调用CreatePeerConnectionFactory创建pc factory
2. 调用CreatePeerConnection创建pc连接
3. 调用addtracks创建和添加本地track

------

![1575339956689](WebRTC手记之框架与接口.assets/1575339956689.png)

1. 传入参数音视频编解码者
2. 调用CreateMediaEngine创建音视频编解码引擎。

------

![1575340254261](WebRTC手记之框架与接口.assets/1575340254261.png)

![1575340322689](WebRTC手记之框架与接口.assets/1575340322689.png)

   ![1575341753465](WebRTC手记之框架与接口.assets/1575341753465.png)

1. 调用GetPeerConnectionString获取stun服务器地址;
2. 调用peer_connection_factory_->CreatePeerConnection创建peerconneciton对象，同时将Conductor类传入当作observer;
3. 调用pc->Initalize初始化pc连接

#### 1.5.2  add stream

![1575344559069](WebRTC手记之框架与接口.assets/1575344559069.png)

1. 调用 peer_connection_factory_->CreateAudioSource创建audiosource
2. 调用peer_connection_factory_->CreateAudioTrack创建audiotrack
3. peer_connection_->AddTrack添加轨迹
4. 同理创建video_track, 然后调用peer_connection_->AddTrack创建轨迹

------

![1575344988211](WebRTC手记之框架与接口.assets/1575344988211.png)

1. 根据track查找对应的Sender, 如果存在着返回，否则进行第二部
2. 调用AddTrackUnifiedPlan或者AddTrackPlanB添加track
3. 调用Stats->AddTrack添加track到状态管理，用于管理各个track的状态

------

![1575345436473](WebRTC手记之框架与接口.assets/1575345436473.png)

AddTrackPlanB和AddTrackUnifiedPlang功能类似，这里拿AddTrackPlanB来分析

1. 根据track的kind属性获取媒体类型
2. 根据类型创建新的sender
3. 创建一个transceiver管理sender

这里有几个概念， **transceiver . sender,  receiver**

**sender 负责编码、发送，receiver 负责接收、解码，一个 sender 至多有一个要发送的 track，一个要接收的 track 刚好有一个 receiver。规范规定 transceiver 包含一个 sender 和一个 receiver，它们拥有相同的 mid，WebRTC 实现的 unified plan 遵循了这一规定。一个 PC 有多个 transceiver。**

**addTrack 时会为其分配 transceiver/sender（新建或复用已有 transceiver/sender），也可以通过 addTransceiver 来建立 track - sender/receiver - transceiver 的关联。**



#### 1.5.3 createoffer

![1575344825077](WebRTC手记之框架与接口.assets/1575344825077.png)

1. 调用peer_connection_->CreateOffer创建Offer，第一个参数为回调函数，继承于CreateSessionDescriptionObserver

2. 通过OnSuccess或者OnFailure获取创建offer的结果

   ![1575354732597](WebRTC手记之框架与接口.assets/1575354732597.png)


3. 调用peer_connection_->SetLocalDescription设置本地sdp
4. 调用SendMessage通过信令通道将sdp信息发送给另一端

#### 1.5.4 createanswer

![1575356280161](WebRTC手记之框架与接口.assets/1575356280161.png)



1. OnMessageFromPeer为信令通道的回调函数，SDP数据的格式为json格式
2. 收到sdp信息后调用peer_connection_->SetRemoteDescription设置远端的SDP信息
3. 如果收到的sdp类型为offer, 则需要创建一个answer回复，回调函数这块和创建offer时一样

#### 1.5.5 ask ice

没找到

#### 1.5.6 OnIceCandidate

![1575357492551](WebRTC手记之框架与接口.assets/1575357492551.png)

1. 将candidate信息转换为json信息
2. 通过信令通道发送candidate信息

这个函数是peerconnection的回调函数，我们跟进去看看

1. 通过OnTransportControllerCandidatesGathered触发的回调
2. 将OnTransportControllerCandidatesGathered注册到transport_controller_
3. transport_controller_的类型为JsepTransportController
4. 通过JsepTransportController::OnTransportCandidateGathered_n触发回调
5. 将OnTransportCandidateGathered_n注册到IceTransportInternal
6. 通过IceTransportInternal的SignalCandidateGathered触发回调

这期间有两个比较重要的概念， **JSEP和DTLS**

**JavaScript会话建立协议（JSEP）提供了供应用程序建立和协商本地和远程会话的接口。这个方法将信令状态机的驱动完全交给了应用程序。应用程序必须在正确的时间使用正确的API并将会话描述信息和ICE（Interactive Connectivity Establishment）信息转换为它所选择的信令协议所期望的消息。**

**DTLS全称为Datagram Transport Layer Security，用于避免UDP传输被窃听、干预和伪造。DTLS基于TLS协议（Transport Layer Security）。**

#### 1.5.7 AddIceCandidate


![1575356651960](WebRTC手记之框架与接口.assets/1575356651960.png)

1.  通过信令通道后收到candidate信息的json格式
2.  调用CreateIceCandidate将string格式转为IceCandidateInterface
3.  调用AddIceCandidate

#### 1.5.8 onAddStream

经过以上几个步骤，p2p来连接就已经建立成功， 再看一下在建立p2p连接过程中是如何加入stream和track的。

因为SDP中是带有流通道信息的，所以在设置SDP信息时可以设置远端stream。

SetRemoteDescription->ApplyRemoteDescription->UpdateRemoteSendersList->OnRemoteSenderAdded->CreateAudioReceiver or  CreateVideoReceiver -> OnAddTrack

OnAddTrack为触发外部的回调函数。

![1575359264316](WebRTC手记之框架与接口.assets/1575359264316.png)

在回调函数中设置该track的RemoteRenderer。



### 1.5 远程视频获取

![1575359386858](WebRTC手记之框架与接口.assets/1575359386858.png)

其中有几个比较重要的类, VideoRtpReceiver、RtpReceiverInternal

![1575362071793](WebRTC手记之框架与接口.assets/1575362071793.png)

以下为远程视频湖获取的流程，其中app代表应用程序

1. 其中先将sink注册到videortptracksource中， videortpreceiver包含videortptracksouce的对象
2. **注意:  注册sink时，都是先调用track的AddOrUpdateSink, 然后track调用构造时绑定的source的AddOrUpdateSink。**
3. 然后在外部触发SetupMediaChannel时，将调用SetSink，将sink注册到WebRtcVideoChannel中
4. 然后再注册到WebRtcVideoReceiveStream中， 通过OnFrame获取视频数据

### 1.6 远程视频渲染

同本地视频渲染，只是视频来源不同，视频来源为rtp。

![1575359264316](WebRTC手记之框架与接口.assets/1575359264316.png)

video_track为onAddTrack传出来的track。

### 1.7 远程视频传送

1.3中已经将视频数据传输到TestVideoCapturer,  只需要看有哪些sink注册到了TestVideoCapturer

![1575366653881](WebRTC手记之框架与接口.assets/1575366653881.png)

调用peer_connection_->AddTrack将本地捕捉track添加到pc中

![1575366815642](WebRTC手记之框架与接口.assets/1575366815642.png)

在AddTrack中可以看到根据track创建了一个Sender， 这个Sender是用来编码和发送视频数据的

![1575366997339](WebRTC手记之框架与接口.assets/1575366997339.png)

在CreateSender中将track和sender绑定了。

![1575426068054](WebRTC手记之框架与接口.assets/1575426068054.png)

如上图所示， 每个Sender都会依赖一个Channel, VideoRtpSender依赖VideoMediaChannel, AudioRtpSender依赖VoiceMediaChannel, RtpSenderBase为基类。

![1575426317283](WebRTC手记之框架与接口.assets/1575426317283.png)

在调用SetTrack时我们可以看到每个sender只能绑定一个track, 如果设置新的track则旧的track就会释放。同时会调用SetSend(), 将track注册到channel中。

![1575426408036](WebRTC手记之框架与接口.assets/1575426408036.png)

video_media_channel() 获取当前sender绑定的通道，video_track()获取绑定的track。

![1575426525647](WebRTC手记之框架与接口.assets/1575426525647.png)

可以看到在SetVideoSend中我们根据ssrc查找指定的send_stream, 然后调用sendStream的SetVideoSend将track注册到其中。

send_streams_的类型如下所示: 

![1575426630209](WebRTC手记之框架与接口.assets/1575426630209.png)

看一下sendstream中SetVideoSend做了什么: 


![1575426687937](WebRTC手记之框架与接口.assets/1575426687937.png)

使用source_= source , 可以看到将source保存到类变量中了， 所以这就将sendstream和track关联起来了

![1575426956378](WebRTC手记之框架与接口.assets/1575426956378.png)

如上图所示， 可以看到将encoder_sink注册到source中，此时sink和track关联起来了，捕捉的视频数据会encoder_sink_输出，从名字中我们可以看出这是用于编码的sink，在编码后再通过RTP将流发送出去。

![1575443683000](WebRTC手记之框架与接口.assets/1575443683000.png)

VideoStreamEncoder作为sink注册到源中，对视频数据进行编码，具体如何注册的不详细跟了。

**总结: 每个channel都有对应得send_stream和recv_stream， 用于和RTP交互， 同时也会有对应得sender和recver， 用于和内部交互**

### 1.8 通道

![1575531514386](WebRTC手记之框架与接口.assets/1575531514386.png)



这张图我觉得是一个非常宏观的图了，我们研究的最多的音视频模块，居然在这张图里，也只是一小部分而已。现在重新来看这张图，回忆当时绘图的思路，总结总结。

peerconection

整个webrtc期间的Session，可以说是webrtc对通话模型的一个抽象，那么在进行webrtc通话的时候，可以有音频，也可以有视频，还可以有一些数据（信息，文件等）。

**BaseChannel**

在webrtc通话的过程中，显然形式是丰富多彩的，如上面介绍的，可以有视频，音频，以及数据。因此webrtc对这些元数据也进行了抽象封装，这就是BaseChannel，即基本通道。这是一个基类，继承这个基类的，有VideoChannel，VoiceChannel以及DataChannel，分别负责传输视频数据，音频数据以及其他数据。

**ChannelManager**

介绍完了BaseChannel，以及其三种多态形式，下面介绍ChannelManager，这其实也是常见的思路，在Android中会经常有类似的命名方式，xxxMaster，xxxManager，xxxHolder等等，其实都是一个控制类。控制什么呢？从名字就可以看出来，就是控制Channel的，究竟是哪个channel呢？就是上面介绍的BaseChannel。再具体一点，实际上上述的三个channel，都是由channelmanager创建出来的。从对象图中我们可以看到这种关系，WebRtcSession有一个成员变量（has-a）ChannelManager，而ChannelManager创建了（create）三种Channel（VoiceChannel，VIdeoChannel以及DataChannel）

**MediaChannel**

下面看看MediaChannel，前面介绍了BaseChannel，以及其三个衍生品，但BaseChannel的主要职责是发送数据，那么数据究竟是怎么产生的呢？我们知道，无论是音频数据，还是视频数据，都要经过一系列的流程才可以产生，比如采集，编码，分包。这个过程显然并不是BaseChannel的职责，专门有对象负责这些事情，就是MediaChannel，也就是媒体通道。由此可见，Webrtc中的通道很多，但是实际上是各司其职，每个通道都由各自的用途。那么MediaChannel和BaseChannel之间的关系是什么样呢？实际上也是has-a的关系，即每个BaseChannel中都有一个MediaChannel。

**WebRtcVideoChannel**

提起WebRtcVideoChannel我们应该再熟悉不过了，相信对于开发webrtc视频底层相关同学，都很熟悉这个对象了，正是这个对象，封装了视频的采集，编解码，以及分包，解包。实际上这个结构体的本质，就是一个MediaChannel，从对象图中我们可以看见这个继承关系。如果要是个性化开发的话，实际上也就是从WebRtcVideoChannel入手就可以了，在52版本上，拿到camera的数据，并和WebRtcVideoChannel挂钩，并且配置WebRtcVideoChannel2的发送/接受码流，以及其参数，视频就可以打通了。

**MediaEngineInterface**

既然介绍了MediaChannel，自然就要想到，究竟是谁创建了MediaChannel呢？就如同上面介绍的，ChannelManager创建了BaseChannel，这里MediaEngineInterface创建了MediaChannel。即媒体引擎创建了媒体通道。这里的媒体引擎，是一个Interface，显然，webrtc的思想是基于抽象进行编程，因此这里提取了Interface层。这种解耦的设计方式也值得我们学习。

**CompositeMediaEngine**

这个对象是MediaEngineInterface的实现类，从命名上来看，实际上是一个组合的媒体引擎，何谓组合呢？就是其有两个成员，其一为WebRtcVoiceEngine，专门负责创建音频的MediaChannel；其二为WebRtcVideoEngine，专门负责创建视频的MediaChannel，也就是WebRtcVideoChannel。这种组合的方式的设计，让整个框架更有弹性，比如其中的一个好处，就是音频，视频引擎可以独立的升级。

**WebRtcMediaEngine**

最后要说说WebRtcMediaEngine了，这个对象我们也应该非常熟悉。实际上初次了解Webrtc媒体相关的同学，可能就是从WebRtcVideoEngine以及WebRtcVideoChannel看起的。但是可能这两个结构体在整个webrtc中的位置就不是很清晰了。从对象图上可以很容易的看出两者的关系，WebRtcMediaEngine本质上是一个MediaEngineInterface，其作用就是为BaseChannel创建MediaChannel。



#### 1.1 到底各个通道之间是如何关联起来的

啊啊啊，概念太多了

![1575527720848](WebRTC手记之框架与接口.assets/1575527720848.png)

如上图所示，在收到远程的SDP信息时，我们会根据SDP信息创建各个通道，包括Voice, Video, Data。

1. 根据sdp信息获取各个通道的mid， 即voice->name、video->name以及data->name
2. 根据mid信息创建对应的通道
3. 将通道和Transceiver绑定, 其中channel会绑定到Transceiver中，而media_channel会绑定到sender和trceiver中。

![1575532085733](WebRTC手记之框架与接口.assets/1575532085733.png)

如上图所示，会调用channel_manager的接口创建通道

1. 根据mid获取一个实际的rtptransport，用于实际的rtp传输。
2. 根据配置船舰一个channel，创建完成后绑定channel和rtp_tramsport。
3. 如何创建channnel呢，看下面

![1575532310013](WebRTC手记之框架与接口.assets/1575532310013.png)

可以看到会调用对应的engine去创建media_channel, 同时构造一个channel

![1575532437634](WebRTC手记之框架与接口.assets/1575532437634.png)

实际创建的媒体通道为WebRtcVoiceEngine。





### 1.9 视频编码和解码

![1575426956378](WebRTC手记之框架与接口.assets/1575426956378.png)

之前我们跟到WebRtcVideoSendStream中，注册了一个encoder_sink_, 那这个sink是什么时候注册的呢，简单的画个框架图

![1575617610708](WebRTC手记之框架与接口.assets/1575617610708.png)

1. 如上图所示: EncoderSimulcassProxy作为videoEncoder的代理，所以的操作都会过它
2. InternalEncoderFactory是实际创建编码器
3. VideoStreamEncoder依赖EncoderSimulcassProxy编码，通过OnEncoderImage回调
4. VideoSendStream实际是通过VideoSendStreamImpl发送数据，VideoStreamEncoder也通过OnEncoderImage回调
5. VideoSendStreamImpl调用MediaChannel中的Transport的SendVideoFrame发送数据。



具体流程看以下几个流程图:


![1575617639101](WebRTC手记之框架与接口.assets/1575617639101.png)

1. 通过一系列操作，将VideoStreamEncoder注册到WebRtcVideoSendStream的sink中
2. VideoStreamEncoder通过OnFrame获取视频数据。



![1575617659661](WebRTC手记之框架与接口.assets/1575617659661.png)

1. VideoStreamEncoder获取视频数据后，根据创建实际的编码者， h264, vp8, vp9

2. 通过OnEncodedImage获取编码后的视频数据。

   


![1575617679704](WebRTC手记之框架与接口.assets/1575617679704.png)



1. 通过一系列操作将VideoSendStreamImpl作为sink注册到VideoStreamEncoder中
2. 通过OnEncodedImage将编码数据传到VideoSendStreamImpl
3. VideoSendStreamImpl调用MediaChannel中的Transport的SendVideoFrame发送数据。

